{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc9f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.3 (SDL 2.0.22, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pygame\n",
    "import os #NEW LINE\n",
    "import gymnasium as gym \n",
    "import tensorflow as tf\n",
    "from stable_baselines3 import PPO, DQN, A2C, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from tensorflow.python.client import device_lib\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c764616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18236890925598444872\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280b1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\",render_mode=\"human\", device=\"cuda\")\n",
    "#env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\",render_mode=\"human\")\n",
    "#env = gym.make(\"FrozenLake-v1\", render_mode=\"human\", device=\"cuda\")\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "# sess.graph contains the graph definition; that enables the Graph Visualizer.\n",
    "file_writer = tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba6ea645",
   "metadata": {},
   "source": [
    "# Adding Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c05639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nt_board_callback = tf.keras.callbacks.TensorBoard(\\n    log_dir=log_path,\\n    histogram_freq=0,\\n    write_graph=True,\\n    write_images=False,\\n    write_steps_per_second=False,\\n    update_freq='epoch',\\n    profile_batch=0,\\n    embeddings_freq=0,\\n    embeddings_metadata=None,\\n)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best=stop_callback, eval_freq=10000, best_model_save_path=save_path, verbose=1)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_path, histogram_freq=1)\n",
    "'''\n",
    "t_board_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_path,\n",
    "    histogram_freq=0,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None,\n",
    ")\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b434cfd",
   "metadata": {},
   "source": [
    "# Pre-training: Testing our environment, Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:0.0\n",
      "Episode:2 Score:0.0\n",
      "Episode:3 Score:0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "episodes = 3\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        #time.sleep(0.01)\n",
    "        action = env.action_space.sample()\n",
    "        #needed to split done into truncated and terminated for it to work. See abvove cell for why\n",
    "        n_state, reward, terminated, truncated, info = env.step(action)\n",
    "        score+=reward\n",
    "        done = truncated or terminated \n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f2313d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space is: Discrete(4)\n",
      "The observation space is: Discrete(16)\n",
      "Sample observation space value: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"The action space is: {}\".format(env.action_space))\n",
    "print(\"The observation space is: {}\".format(env.observation_space))\n",
    "print(\"Sample observation space value: {}\".format(env.observation_space.sample()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cf367c7",
   "metadata": {},
   "source": [
    "# Model 1 Evaluation: PPO Algorithm, No GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4e9896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\",map_name=\"8x8\")\n",
    "#Reinterprets the env\n",
    "env = DummyVecEnv([lambda: env])\n",
    "#Defines the 'agent'\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed26ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985c5e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_13\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3    |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 530  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1062        |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004726704 |\n",
      "|    clip_fraction        | 0.00928     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -21.5       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00226    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00697    |\n",
      "|    value_loss           | 0.00316     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1594        |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012079382 |\n",
      "|    clip_fraction        | 0.0885      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -1.43       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 0.000169    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3           |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2128        |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012754638 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | -4.25       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 6.21e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      3\u001b[0m \u001b[39m#Re-running this training will accumulate\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtimesteps, callback\u001b[39m=\u001b[39;49meval_callback)\n\u001b[0;32m      5\u001b[0m stop \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPPO: Total Training time for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m timesteps : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(timesteps, stop\u001b[39m-\u001b[39mstart))\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:308\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    300\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    301\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    309\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    310\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    311\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    313\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    314\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    315\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    183\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[1;32m--> 184\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:449\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success_buffer \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 449\u001b[0m episode_rewards, episode_lengths \u001b[39m=\u001b[39m evaluate_policy(\n\u001b[0;32m    450\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env,\n\u001b[0;32m    452\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    453\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender,\n\u001b[0;32m    454\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeterministic,\n\u001b[0;32m    455\u001b[0m     return_episode_rewards\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    456\u001b[0m     warn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn,\n\u001b[0;32m    457\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_success_callback,\n\u001b[0;32m    458\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluations_timesteps\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:84\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     82\u001b[0m current_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_envs)\n\u001b[0;32m     83\u001b[0m current_lengths \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_envs, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m observations \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     85\u001b[0m states \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:86\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvObs:\n\u001b[0;32m     85\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 86\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     87\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:322\u001b[0m, in \u001b[0;36mFrozenLakeEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms), {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:408\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    405\u001b[0m pos \u001b[39m=\u001b[39m (x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m0\u001b[39m], y \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m1\u001b[39m])\n\u001b[0;32m    406\u001b[0m rect \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39mpos, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size)\n\u001b[1;32m--> 408\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_surface\u001b[39m.\u001b[39;49mblit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mice_img, pos)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m desc[y][x] \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface\u001b[39m.\u001b[39mblit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhole_img, pos)\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "timesteps = 10000\n",
    "start = time.time()\n",
    "#Re-running this training will accumulate\n",
    "model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "stop = time.time()\n",
    "print(\"PPO: Total Training time for {} timesteps : {}s\".format(timesteps, stop-start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2127e0d",
   "metadata": {},
   "source": [
    "# Saving, Teardown, and Reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c902da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join(save_path, 'PPO_model_frozen_8x8')\n",
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518f603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 Evaluation:\n",
    "evaluate_policy(model, env, n_eval_episodes=12, render=True)\n",
    "env.close()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45141e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Recovery Point after the close()\n",
    "PPO_path = os.path.join(save_path, 'PPO_model_frozen_8x8')\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\",map_name=\"8x8\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf14fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info [{'prob': 0.3333333333333333, 'TimeLimit.truncated': False, 'terminal_observation': 19}]\n"
     ]
    }
   ],
   "source": [
    "# Model 1 Test\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b0b6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For Troubleshooting Display Error:\n",
    "del model\n",
    "'''\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32f05ba5",
   "metadata": {},
   "source": [
    "# Model 2 Evaluation: DQN Algorithm; No GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAY NEED TO BREAK HEAR, RESTART FROM LOAD IN NEXT CELL\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\",map_name=\"8x8\")\n",
    "#Reinterprets the env\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-running this training will accumulate\n",
    "timesteps = 20000\n",
    "start = time.time()\n",
    "model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "stop = time.time()\n",
    "print(\"DQN: Total Training time for {} timesteps : {}s\".format(timesteps, stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ff2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_path = os.path.join(save_path, 'DQN_model_frozen')\n",
    "model.save(DQN_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5543e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=12, render=True)\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe799fae",
   "metadata": {},
   "source": [
    "# Model 3 Evaluation: A2C Algorithm; No GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b92bbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "#MAY NEED TO BREAK HEAR, RESTART FROM LOAD IN NEXT CELL\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\",map_name=\"8x8\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = A2C('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5327244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\A2C_5\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -4.56    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00316  |\n",
      "|    value_loss         | 8.33e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 256       |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.52e-05 |\n",
      "|    value_loss         | 1.47e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 385      |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -738     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0095   |\n",
      "|    value_loss         | 6.05e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 514      |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | -5.54    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0359  |\n",
      "|    value_loss         | 0.00129  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 643       |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -1.07e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 0.0137    |\n",
      "|    value_loss         | 0.000132  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 772       |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -6.73e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 0.00239   |\n",
      "|    value_loss         | 6.42e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 902      |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.38    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0138   |\n",
      "|    value_loss         | 0.000109 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 1032      |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.38     |\n",
      "|    explained_variance | -4.82e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.00692  |\n",
      "|    value_loss         | 4.07e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 1162      |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.36     |\n",
      "|    explained_variance | -3.05e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.148     |\n",
      "|    value_loss         | 0.0148    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 1292      |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.34     |\n",
      "|    explained_variance | -1.98e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.00648  |\n",
      "|    value_loss         | 3.59e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 1421     |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | -2.2e+03 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00792 |\n",
      "|    value_loss         | 7.27e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 1551      |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.33     |\n",
      "|    explained_variance | -7.24e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.00618  |\n",
      "|    value_loss         | 0.00113   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 1680     |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.34    |\n",
      "|    explained_variance | -1e+03   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0304  |\n",
      "|    value_loss         | 0.000759 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 1809      |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32     |\n",
      "|    explained_variance | -6.24e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.0198   |\n",
      "|    value_loss         | 0.000512  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 3        |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 1939     |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.33    |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000221 |\n",
      "|    value_loss         | 3.1e-08  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 2071      |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.28     |\n",
      "|    explained_variance | -2.99e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.00249  |\n",
      "|    value_loss         | 0.000213  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 2202      |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.24     |\n",
      "|    explained_variance | -6.88e+03 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.00363  |\n",
      "|    value_loss         | 5.06e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 2333      |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.19     |\n",
      "|    explained_variance | -1.37e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.00364  |\n",
      "|    value_loss         | 1.2e-05   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 3         |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 2464      |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.3      |\n",
      "|    explained_variance | -5.82e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.000907  |\n",
      "|    value_loss         | 1.52e-05  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m timesteps \u001b[39m=\u001b[39m \u001b[39m20000\u001b[39m\n\u001b[0;32m      3\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtimesteps, callback\u001b[39m=\u001b[39;49meval_callback)\n\u001b[0;32m      5\u001b[0m stop \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mA2C: Total Training time for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m timesteps : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(timesteps, stop\u001b[39m-\u001b[39mstart))\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:194\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    186\u001b[0m     \u001b[39mself\u001b[39m: SelfA2C,\n\u001b[0;32m    187\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    193\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    195\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    196\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    197\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    198\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    199\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    200\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    201\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:259\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 259\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:184\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    183\u001b[0m callback\u001b[39m.\u001b[39mupdate_locals(\u001b[39mlocals\u001b[39m())\n\u001b[1;32m--> 184\u001b[0m \u001b[39mif\u001b[39;00m callback\u001b[39m.\u001b[39;49mon_step() \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_info_buffer(infos)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:104\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_calls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_step()\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\callbacks.py:449\u001b[0m, in \u001b[0;36mEvalCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39m# Reset success rate buffer\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_success_buffer \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 449\u001b[0m episode_rewards, episode_lengths \u001b[39m=\u001b[39m evaluate_policy(\n\u001b[0;32m    450\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[0;32m    451\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval_env,\n\u001b[0;32m    452\u001b[0m     n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    453\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender,\n\u001b[0;32m    454\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeterministic,\n\u001b[0;32m    455\u001b[0m     return_episode_rewards\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    456\u001b[0m     warn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn,\n\u001b[0;32m    457\u001b[0m     callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_success_callback,\n\u001b[0;32m    458\u001b[0m )\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluations_timesteps\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:84\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     82\u001b[0m current_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_envs)\n\u001b[0;32m     83\u001b[0m current_lengths \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(n_envs, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mint\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 84\u001b[0m observations \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     85\u001b[0m states \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mnum_envs,), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:86\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvObs:\n\u001b[0;32m     85\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 86\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m     87\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m    The reset environment\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:57\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m env_reset_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:322\u001b[0m, in \u001b[0;36mFrozenLakeEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastaction \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms), {\u001b[39m\"\u001b[39m\u001b[39mprob\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:338\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[1;32mc:\\Users\\limon\\anaconda3\\envs\\gym_vrp\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:408\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    405\u001b[0m pos \u001b[39m=\u001b[39m (x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m0\u001b[39m], y \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size[\u001b[39m1\u001b[39m])\n\u001b[0;32m    406\u001b[0m rect \u001b[39m=\u001b[39m (\u001b[39m*\u001b[39mpos, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcell_size)\n\u001b[1;32m--> 408\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow_surface\u001b[39m.\u001b[39;49mblit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mice_img, pos)\n\u001b[0;32m    409\u001b[0m \u001b[39mif\u001b[39;00m desc[y][x] \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    410\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_surface\u001b[39m.\u001b[39mblit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhole_img, pos)\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "#Re-running this training will accumulate\n",
    "timesteps = 20000\n",
    "start = time.time()\n",
    "model.learn(total_timesteps=timesteps, callback=eval_callback)\n",
    "stop = time.time()\n",
    "print(\"A2C: Total Training time for {} timesteps : {}s\".format(timesteps, stop-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0229278",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_path = os.path.join('Training', 'Saved Models', 'A2C_model_frozen_8x8')\n",
    "model.save(A2C_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cc2c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=12, render=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
